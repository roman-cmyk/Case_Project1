{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Funciones_EDA_ETL_Datos_Global_News_V2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4XfjOp2iuwv"
      },
      "source": [
        "##Time Stamp Funcion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOVRDDqTiySz"
      },
      "source": [
        "def get_timestampYMD():\n",
        "    import datetime\n",
        "    return datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooKo0LhMi67N"
      },
      "source": [
        "##Load Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrxV9Dm3i_LT"
      },
      "source": [
        "def define_stopwords():\n",
        "    sw_es=stopwords.words('spanish')\n",
        "    sw_pt=stopwords.words('portuguese')\n",
        "    sw_en=stopwords.words('english')\n",
        "    sw_es_title=[word.title() for word in sw_es]\n",
        "    sw_pt_title=[word.title() for word in sw_pt]\n",
        "    sw_en_title=[word.title() for word in sw_en]\n",
        "    return sw_es, sw_pt, sw_en, sw_es_title, sw_pt_title, sw_en_title"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b93xWnusjHpn"
      },
      "source": [
        "##Extract Uppercase Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P4tklaJjCPY"
      },
      "source": [
        "def get_mayusculas(texto):\n",
        "    texto=str(texto)\n",
        "    palabras_importantes=[]\n",
        "    mayusculas=(r\"([A-Z][a-zÁ-ÿ0-9]{1,20}\\s?\\,?\\.?\\s?)\")\n",
        "    texto=re.sub('[^\\w\\s]',' ',texto)\n",
        "    texto=re.sub('[0-9]+', '', texto)  \n",
        "    tokenizer=RegexpTokenizer(r'\\w+')\n",
        "    texto=tokenizer.tokenize(texto)\n",
        "    texto=[word for word in texto if word not in sw_pt]\n",
        "    texto=[word for word in texto if word not in sw_es]\n",
        "    texto=[word for word in texto if word not in sw_en]\n",
        "    texto=[word for word in texto if word not in sw_pt_title]\n",
        "    texto=[word for word in texto if word not in sw_es_title]\n",
        "    texto=[word for word in texto if word not in sw_en_title]\n",
        "    tokens=[word.strip() for word in texto if word is not None]\n",
        "    tokens=[word.strip() for word in tokens if len(word)>1]\n",
        "    palabras_importantes=[word for word in tokens if word.istitle()]\n",
        "    return palabras_importantes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4SwD-n-jYeU"
      },
      "source": [
        "##Adjust Country & Subsidiary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIUTUDymjXrm"
      },
      "source": [
        "def country_sub(country):\n",
        "    if country =='Panama' or country =='PA' or country=='panama' or country=='Panamá':\n",
        "        return ['Panama','Central']\n",
        "    elif country =='Costa Rica' or country =='CR' or country=='costa rica':\n",
        "        return ['Costa Rica','Central']\n",
        "    elif country =='Honduras' or country =='HN' or country=='honduras':\n",
        "        return ['Honduras','Central']\n",
        "    elif country =='Nicaragua' or country =='NI' or country=='nicaragua':\n",
        "        return ['Nicaragua','Central']\n",
        "    elif country =='Guatemala' or country =='GT' or country=='guatemala':\n",
        "        return ['Guatemala','Central']\n",
        "    elif country =='El Salvador' or country =='SV' or country=='el salvador':\n",
        "        return ['El Salvador','Central']\n",
        "    elif country =='Venezuela' or country =='VE' or country=='venezuela':\n",
        "        return ['Venezuela','Central']\n",
        "    elif country =='Puerto Rico' or country =='PR' or country=='puerto rico':\n",
        "        return ['Puerto Rico','Caribbean']\n",
        "    elif country =='Dominican Republic' or country =='DO' or country=='dominican republic' or country=='republica dominicana' or country=='Rep. Dominicana':\n",
        "        return ['Dominican Republic','Caribbean']\n",
        "    elif country =='Trinidad & Tobago' or country =='TT' or country=='trinidad & tobago' or country=='Trinidad y Tobago' or country=='T&T':\n",
        "        return ['Trinidad & Tobago','Caribbean']\n",
        "    elif country =='Jamaica' or country =='JM' or country=='jamaica':\n",
        "        return ['Jamaica','Caribbean']\n",
        "    elif country =='Peru' or country =='PE' or country=='peru' or country=='Perú':\n",
        "        return ['Peru','South']\n",
        "    elif country =='Ecuador' or country =='EC' or country=='ecuador' or country=='equador' or country=='EQUADOR':\n",
        "        return ['Ecuador','South']\n",
        "    elif country =='Bolivia' or country =='BO' or country=='bolivia':\n",
        "        return ['Bolivia','South']\n",
        "    elif country =='Paraguay' or country =='PY' or country=='paraguay' or country=='paraguai' or country=='Paraguai':\n",
        "        return ['Paraguay','South']\n",
        "    elif country =='Uruguay' or country =='UY' or country=='uruguay'or country=='uruguai' or country=='Uruguai':\n",
        "        return ['Uruguay','South']\n",
        "    elif country =='Mexico' or country =='MX' or country=='mexico' or country=='México' or country=='méxico':\n",
        "        return ['Mexico','Mexico']\n",
        "    elif country =='PanLatam' or country =='panlatam' or country =='Latinoamérica':\n",
        "        return ['PanLatam','Panlatam']\n",
        "    elif country =='PanCentral' or country =='pancentral':\n",
        "        return ['PanCentral','Panlatam']\n",
        "    elif country =='Argentina' or country =='AR' or country=='argentina':\n",
        "        return ['Argentina','Argentina']\n",
        "    elif country =='Brazil' or country =='BR' or country=='brazil' or country=='brasil' or country=='Brasil':\n",
        "        return ['Brazil','Brazil']\n",
        "    elif country =='Chile' or country =='CL' or country=='chile':\n",
        "        return ['Chile','Chile']\n",
        "    elif country =='Colombia' or country =='CL' or country=='colombia':\n",
        "        return ['Colombia','Colombia']\n",
        "    else:\n",
        "        return ['Revisar','Revisar']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJUxcp9Ijs7F"
      },
      "source": [
        "##Adjust Month & Quarters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt_WSfcCjqLm"
      },
      "source": [
        "def find_quarters(month):\n",
        "    if month =='1': \n",
        "        return ['January','07','Q3','H2']\n",
        "    elif month =='2': \n",
        "        return ['February','08','Q3','H2']\n",
        "    elif month =='3': \n",
        "        return ['March','09','Q3','H2']\n",
        "    elif month =='4': \n",
        "        return ['April','10','Q4','H2']\n",
        "    elif month =='5': \n",
        "        return ['May','11','Q4','H2']\n",
        "    elif month =='6': \n",
        "        return ['June','12','Q4','H2']\n",
        "    elif month =='7': \n",
        "        return ['July','01','Q1','H2']\n",
        "    elif month =='8': \n",
        "        return ['August','02','Q1','H2']\n",
        "    elif month =='9': \n",
        "        return ['September','03','Q1','H2']\n",
        "    elif month =='10': \n",
        "        return ['October','04','Q2','H2']\n",
        "    elif month =='11': \n",
        "        return ['November','05','Q2','H2']\n",
        "    elif month =='12': \n",
        "        return ['December','06','Q2','H2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcbLjpybj4Na"
      },
      "source": [
        "##Remove Punctuation Marks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8dq_vedj0vN"
      },
      "source": [
        "def remove_punct(texto):\n",
        "    try:\n",
        "        texto=texto.replace(\".\",' ').replace(\";\",' ').replace(\":\",' ').replace(\",\",' ')\n",
        "        texto=texto.replace(\"(\",' ').replace(\")\",' ').replace(\"|\",' ').replace('\"',' ')\n",
        "        texto=texto.replace(\"%\",' ').replace(\"$\",' ').replace(\"/\",' ').replace('\\'',' ')\n",
        "        texto=texto.replace(\"-\",' ').replace(\"_\",' ').replace(\"*\",' ').replace('+',' ')\n",
        "        texto=texto.replace(\"#\",' ').replace(\"@\",' ').replace(\"!\",' ').replace('?',' ')\n",
        "    except:\n",
        "        pass\n",
        "    return texto "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJV5GxkTkEe4"
      },
      "source": [
        "##Convert Text To Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5lhigUykA8P"
      },
      "source": [
        "def clean_text(texto):\n",
        "    texto=texto.lower()\n",
        "    texto=remove_punct(texto)\n",
        "    return texto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFZcx7CykMiL"
      },
      "source": [
        "##Text Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU1l6-7ukLjJ"
      },
      "source": [
        "def tokenizar(texto):\n",
        "    tokens = [t for t in texto.split()]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4H_UZd8kSqA"
      },
      "source": [
        "##Build Keywords Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFJBrKNzkTCu"
      },
      "source": [
        "def crear_dicc_keywords(df_keywords):\n",
        "    df_keywords=df_keywords.fillna('exxxtract')\n",
        "    area_dict = df_keywords.to_dict('list')\n",
        "    for k,v in area_dict.items():\n",
        "        nv=list(set(v))\n",
        "        nv=[x for x in v if x != 'exxxtract']\n",
        "        nv=list(set(nv))\n",
        "        area_dict[k]=nv\n",
        "    return area_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsIrlOl0kuh3"
      },
      "source": [
        "##Convert Identified Words To a List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSiKLJwCkhZi"
      },
      "source": [
        "def convert_to_words(list):\n",
        "    new_list=[]\n",
        "    for item,item2 in zip(list,category_list_found):\n",
        "        if int(item)>0: \n",
        "            new_list.append(item2.replace('_FOUND',''))\n",
        "    return new_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbLU0t03lV6b"
      },
      "source": [
        "##Read .JSON Files With Encoding 'UTF-8'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ5lpgWIlWDU"
      },
      "source": [
        "def read_json(file_path):\n",
        "    data={}\n",
        "    with open(file_path,encoding=\"utf8\") as json_file:\n",
        "        arq = json.load(json_file)\n",
        "    data['Notas']=arq\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3efftMIalz3g"
      },
      "source": [
        "##Create Empty DataFrame To Save Global News Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mVwBOQsl0AC"
      },
      "source": [
        "def crear_df_json(data):                                                                                  \n",
        "    df_n=pd.DataFrame(columns=['IdNoticia', 'Fecha', 'Hora', 'TipoDeMedio', 'Medio', 'País', 'Sección','Título', 'Cuerpo', 'Tier',\n",
        "                               'NroCaracteres', 'Tono', 'LinkImagen','CPE', 'Moneda', 'Audiencia', 'Tema', 'Empresa', 'NroPagina',\n",
        "                               'Dimensión', 'CirculacionMedio', 'AutorConductor', 'ResumenAclaracion','LinkNota'])\n",
        "    for item in data['Notas']:                                                                            \n",
        "        dftemp=pd.DataFrame.from_dict(item,orient='index')                                                \n",
        "        dftemp=dftemp.T                                                                                   \n",
        "        df_n=df_n.append(dftemp)                                                                          \n",
        "    df_n=df_n.reset_index()                                                                               \n",
        "    return df_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnOKaymwm5Me"
      },
      "source": [
        "##Find Out a Single Word In Tokenized Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHCct2lcm5Yk"
      },
      "source": [
        "def find_single_word_in_tokenized_text(row,keyword):\n",
        "    if len([x for x in row if x ==keyword])>=1: \n",
        "        return [x for x in row if x ==keyword] \n",
        "    else:\n",
        "        return ' ' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZn6kELpnOrO"
      },
      "source": [
        "##Function To Categorize Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH2H7VwEnO5C"
      },
      "source": [
        "def categoriza_nota(Brands,Priority_products,text):\n",
        "    text=str(text)\n",
        "    text=text.replace('[','').replace(']','')\n",
        "    menciones=re.findall(Brands,text)\n",
        "    if menciones:\n",
        "        dicc_menciones={}\n",
        "        dicc_menciones[list(set(menciones))[0]]=len(menciones)\n",
        "    else:\n",
        "        dicc_menciones={}\n",
        "    productos_enc=[]\n",
        "    for item in Priority_products:\n",
        "        if item:\n",
        "            p_enc=re.findall(item,text)\n",
        "            productos_enc.append(p_enc)\n",
        "        else:\n",
        "            None\n",
        "    dicc={}\n",
        "    for item2 in productos_enc:\n",
        "        if item2:\n",
        "            dicc[list(set(item2))[0]]=len(item2)\n",
        "        else:\n",
        "            None\n",
        "    flat_products = [x for sublist in productos_enc for x in sublist]\n",
        "    cuenta=len(menciones)+len(flat_products)\n",
        "    if cuenta>=3:\n",
        "        cat_nota='Prominent'\n",
        "    elif cuenta>1 and cuenta<3:\n",
        "        cat_nota='Relevant'\n",
        "    elif cuenta==1:\n",
        "        cat_nota='Passive'\n",
        "    else:\n",
        "        cat_nota='Non_related'\n",
        "    return dicc_menciones,dicc,cat_nota\n",
        "\n",
        "# Mas de 3 menciones de producto+marca = Prominent\n",
        "# 1<mención de Marca + Producto<3 = Relevant\n",
        "# 1=Pasive\n",
        "# 0=Null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-pTAwV1oTZP"
      },
      "source": [
        "##Define User Agent List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsi4OrFioGPH"
      },
      "source": [
        "import urllib.request\n",
        "import random\n",
        "\n",
        "def define_user_agent_list():\n",
        "    user_agent_list = [\n",
        "       #Chrome\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
        "        #Firefox\n",
        "        'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
        "        'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
        "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
        "        'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
        "        'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
        "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
        "        'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
        "        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
        "        'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
        "        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
        "        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
        "        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
        "    ]\n",
        "    return user_agent_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzXjSxIroYgO"
      },
      "source": [
        "##Get Full Content Using Web Scraping With BeautifulSoup & Urllib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWrDei5noYpQ"
      },
      "source": [
        "def get_full_content(url):\n",
        "    for i in range(1,6):\n",
        "    #Pick a random user agent\n",
        "        user_agent = random.choice(user_agent_list)\n",
        "        #Set the headers \n",
        "        headers = {'User-Agent': user_agent}\n",
        "\n",
        "        if url:\n",
        "            try: \n",
        "                req = urllib.request.Request(url,headers={'User-Agent': user_agent})\n",
        "                response = urllib.request.urlopen(req)\n",
        "                html = response.read()\n",
        "                 \n",
        "                soup = BeautifulSoup(html)\n",
        "                paragraphs=re.findall(r'<p>(.*?)</p>',str(soup))\n",
        "                paragraphs=re.sub(r'<a href=.+?(?=)>|</a>|\\xa0|<strong>|</strong>|<i(.*?)</i>|<img(.*?)>','',str(paragraphs))\n",
        "                paragraphs2=''.join(paragraphs)\n",
        "                paragraphs2=paragraphs2.replace('\\n','').replace('\\t','').replace('\\r','')\n",
        "            except:\n",
        "                paragraphs2='Sin Informacion'\n",
        "                print('ups')\n",
        "\n",
        "    return(paragraphs2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtjeedyUo8at"
      },
      "source": [
        "##Clean & Tokenize Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gblRtpMuo8km"
      },
      "source": [
        "def clean_and_tokenize(texto):\n",
        "    texto=str(texto).lower()\n",
        "    texto=re.sub('[^\\w\\s]',' ',texto)\n",
        "    tokenizer=RegexpTokenizer(r'\\w+')\n",
        "    texto=tokenizer.tokenize(texto)\n",
        "    texto=[word for word in texto if word not in sw_pt]\n",
        "    texto=[word for word in texto if word not in sw_es] #Limpiar stopwords en español\n",
        "    texto=[word for word in texto if word not in swsp] #Limpiar stopwords en español\n",
        "    tokens=[word.strip() for word in texto if word is not None]\n",
        "    tokens=[word.strip() for word in tokens if len(word)>1]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtJNHRY_pNX2"
      },
      "source": [
        "##Find Most Common Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A7atjBTpNfi"
      },
      "source": [
        "def find_most_common(texto,filtradas,qty):\n",
        "    tokens=clean_and_tokenize(texto)\n",
        "    tokens2=[word for word in tokens if word not in filtradas]\n",
        "    bigrm = list(nltk.bigrams(tokens2))\n",
        "    found=collections.Counter(bigrm)\n",
        "    tops=found.most_common(qty)\n",
        "    mf=[]\n",
        "    for i in range(0,len(tops)):\n",
        "        terms=tops[i][0]\n",
        "        freq=tops[i][1]\n",
        "        most_frequent=(terms,freq)\n",
        "        mf.append(most_frequent)\n",
        "    return mf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzCjA4yFqF62"
      },
      "source": [
        "##Find Microsoft Most Common Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvT2VfPPqGCU"
      },
      "source": [
        "def MC_WORDS(df,filtro_company,qty):\n",
        "    filtradas=[]\n",
        "    df['Contents_MC']=df['Contents'].apply(lambda row : str(row).replace('http://','').replace('https://','').replace('www',''))\n",
        "    for filtro in filtro_company:\n",
        "        df['Contents_MC']=df['Contents'].apply(lambda row : str(row).replace(filtro,''))\n",
        "    lista02=df['Contents'].tolist()\n",
        "    mc03=find_most_common(lista02,filtradas,qty)\n",
        "    return mc03"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oHv3hrrqjpo"
      },
      "source": [
        "def MC_WORDS2(df,filtro_company,qty):\n",
        "    filtradas=[]\n",
        "    df['Full Text_MC']=df['Full Text'].apply(lambda row : str(row).replace('http://','').replace('https://','').replace('www',''))\n",
        "    for filtro in swsp:\n",
        "        df['Full Text_MC']=df['Full Text'].apply(lambda row : str(row).replace(filtro,''))\n",
        "    lista02=df['Full Text_MC'].tolist()\n",
        "    mc03=find_most_common(lista02,filtradas,qty)\n",
        "    return mc03"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8HGwnMWqu_N"
      },
      "source": [
        "##Define Spanish Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfeEHKPUqoZh"
      },
      "source": [
        "def define_swsp():\n",
        "\n",
        "    swsp=['de','la','que','el','en','y','a','los','del','se','las','por','un','para','con','no',\n",
        "          'una','su','al','lo','como','más','pero','sus','le','ya','o','este','sí','porque','esta',\n",
        "          'entre','cuando','muy','sin','sobre','también','me','hasta','hay','donde','quien','desde',\n",
        "          'todo','nos','durante','todos','uno','les','ni','contra','otros','ese','eso','ante','ellos',\n",
        "          'e','esto','mí','antes','algunos','qué','unos','yo','otro','otras','otra','él','tanto','esa',\n",
        "          'estos','mucho','quienes','nada','muchos','cual','poco','ella','estar','estas','algunas','algo',\n",
        "          'nosotros','mi','mis','tú','te','ti','tu','tus','ellas','nosotras','vosostros','vosostras','os',\n",
        "          'mío','mía','míos','mías','tuyo','tuya','tuyos','tuyas','suyo','suya','suyos','suyas','nuestro',\n",
        "          'nuestra','nuestros','nuestras','vuestro','vuestra','vuestros','vuestras','esos','esas','estoy',\n",
        "          'estás','está','estamos','estáis','están','esté','estés','estemos','estéis','estén','estaré',\n",
        "          'estarás','estará','estaremos','estaréis','estarán','estaría','estarías','estaríamos','estaríais',\n",
        "          'estarían','estaba','estabas','estábamos','estabais','estaban','estuve','estuviste','estuvo',\n",
        "          'estuvimos','estuvisteis','estuvieron','estuviera','estuvieras','estuviéramos','estuvierais',\n",
        "          'estuvieran','estuviese','estuvieses','estuviésemos','estuvieseis','estuviesen','estando','estado',\n",
        "          'estada','estados','estadas','estad','he','has','ha','hemos','habéis','han','haya','hayas',\n",
        "          'hayamos','hayáis','hayan','habré','habrás','habrá','habremos','habréis','habrán','habría',\n",
        "          'habrías','habríamos','habríais','habrían','había','habías','habíamos','habíais','habían','hube',\n",
        "          'hubiste','hubo','hubimos','hubisteis','hubieron','hubiera','hubieras','hubiéramos','hubierais',\n",
        "          'hubieran','hubiese','hubieses','hubiésemos','hubieseis','hubiesen','habiendo','habido','habida',\n",
        "          'habidos','habidas','soy','eres','es','somos','sois','son','sea','seas','seamos','seáis','sean',\n",
        "          'seré','serás','será','seremos','seréis','serán','sería','serías','seríamos','seríais','serían',\n",
        "          'era','eras','éramos','erais','eran','fui','fuiste','fue','fuimos','fuisteis','fueron','fuera',\n",
        "          'fueras','fuéramos','fuerais','fueran','fuese','fueses','fuésemos','fueseis','fuesen','sintiendo',\n",
        "          'sentido','sentida','sentidos','sentidas','siente','sentid','tengo','tienes','tiene','tenemos',\n",
        "          'tenéis','tienen','tenga','tengas','tengamos','tengáis','tengan','tendré','tendrás','tendrá',\n",
        "          'tendremos','tendréis','tendrán','tendría','tendrías','tendríamos','tendríais','tendrían','tenía',\n",
        "          'tenías','teníamos','teníais','tenían','tuve','tuviste','tuvo','tuvimos','tuvisteis','tuvieron',\n",
        "          'tuviera','tuvieras','tuviéramos','tuvierais','tuvieran','tuviese','tuvieses','tuviésemos',\n",
        "          'tuvieseis','tuviesen','teniendo','tenido','tenida','tenidos','tenidas','tened','https','co','mucha','rt','poner',\n",
        "          'interlocutor','interlocutora','presidente','AMLO','Andrés Manuel López Obrador','Andres Manuel Lopez Obrador',\n",
        "          'Andres Manuel','Lopez Obrador','Andrés Manuel','López Obrador','PRESIDENTE ANDRÉS MANUEL LÓPEZ OBRADOR',\n",
        "          'INTERLOCUTORA','INTERLOCUTOR','andrés','manuel','lópez','obrador','interlocutor','interlocutora','br']\n",
        "    return swsp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gralym9rG_1"
      },
      "source": [
        "##Remove Spanish Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx3B3NHIrHLM"
      },
      "source": [
        "def remove_swsp(texto):                    \n",
        "    tokens = [t for t in texto.split()]\n",
        "    clean_tokens = tokens[:]\n",
        "    for token in tokens:\n",
        "        if token in swsp:\n",
        "            clean_tokens.remove(token)\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WKBlknZyCAE"
      },
      "source": [
        "##Split The Text In N-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkJ-aFVnyCJL"
      },
      "source": [
        "def clean_text_wt(texto):\n",
        "    clean_tokens=clean_and_tokenize(texto)\n",
        "    texto = ' '.join(clean_tokens)\n",
        "    return texto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgIXib9xyUlC"
      },
      "source": [
        "def divide_en_ngramas(texto,n):\n",
        "    if len(texto)>0:\n",
        "        n_grams=[]\n",
        "        enegramas = ngrams(texto.split(), n)\n",
        "        for grams in enegramas:\n",
        "            i=0\n",
        "            division=''\n",
        "            while i<n: \n",
        "                division=division+' '+grams[i]\n",
        "                i+=1\n",
        "            n_grams.append(division.lstrip())\n",
        "    else:\n",
        "        return ''\n",
        "    return n_grams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTrEj1j-yeDR"
      },
      "source": [
        "##Detect N-Grams In Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n80HXCdPyeLS"
      },
      "source": [
        "def detect_ngram_in_text(texto,keyword):\n",
        "    tam_keyword=len(keyword.split(' '))\n",
        "    if tam_keyword==1:\n",
        "\n",
        "        texto_fuente=clean_and_tokenize(texto)\n",
        "        if keyword in texto_fuente:\n",
        "            return 1\n",
        "        else: \n",
        "            return 0\n",
        "    else:\n",
        "        texto_fuente=divide_en_ngramas(texto,tam_keyword)\n",
        "        if keyword in texto_fuente:\n",
        "            return 1\n",
        "        else: \n",
        "            return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRI6o_gAyo6L"
      },
      "source": [
        "def detect_ngram_in_text2(texto,keyword):\n",
        "    tam_keyword=len(keyword.split(' '))\n",
        "    if tam_keyword==1:\n",
        "\n",
        "        texto_fuente=clean_and_tokenize(texto)\n",
        "        if keyword in texto_fuente:\n",
        "            return 1\n",
        "        else: \n",
        "            return 0\n",
        "    else:\n",
        "        texto_fuente=divide_en_ngramas(texto,tam_keyword)\n",
        "        if keyword in texto_fuente:\n",
        "            return 1\n",
        "        else: \n",
        "            return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lZq4S9yxgS"
      },
      "source": [
        "##Find Out Best Matches Between Keywords & Words In Content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvcwhjpJyxrV"
      },
      "source": [
        "def encuentra_matches(row,keyword):\n",
        "    row=str(row)\n",
        "    matched=process.extract(keyword,row.split(' '))\n",
        "    best_options=[]\n",
        "    for item in matched:\n",
        "        if item[1]>=91:\n",
        "            best_options.append(item[0])\n",
        "    return best_options"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSmKy3Fkz4_R"
      },
      "source": [
        "def find_keyword_in_clean_text(row,keyword):\n",
        "    patterns= [keyword]  \n",
        "    for p in patterns:\n",
        "        match= re.findall(p, row)\n",
        "    return match"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn8YXqrO0CLC"
      },
      "source": [
        "##Define Words To Verify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pSTqUKEz5u7"
      },
      "source": [
        "def define_words_to_verify():\n",
        "    words_to_verify=['Microsoft','Office 365', 'Bing', 'Cortana', 'Dynamics', 'Azure Cosmos', 'O365',  'Microsoft Cognitive',  'Internet Explorer',  'Minecraft',\n",
        "                     'Windows Server','Github','GitHub','Scarlett','Power BI', 'Edge', 'OneNote',  'PowerPoint',  'Windows',  'Power Point', \n",
        "                     'Office', 'Cortana Intelligence Suite', 'Power Apps', 'LinkedIn', 'Age of Empires','ID@Xbox','SharePoint', 'Xbox Project Scarlett', 'Xbox One','Xbox Scarlett',\n",
        "                     'Surface Pro', 'Surface', 'Azure', 'OneDrive', 'Outlook', 'SQL Server', 'Xbox One','Microsoft Flight Simulator', 'Halo', 'Word',\n",
        "                     'Paint',  'Excel',  'Xbox',  'One Drive',  'Azure Al',  'HoloLens',  'Microsoft Bot Framework',  'Teams', 'TEAMS','Windows Defender',\n",
        "                     'Skype', 'Skype for Business','Apple','iPad','Mac','OSX','Siri','iPhone','iMac','HomePod',\n",
        "                     'ARKit','MacOS','Macbook','watchOS','Airpods','FaceTime','tvOS','SwiftUI', 'Amazon','Amazon Web Services',\n",
        "                     'AWS','AWS Educate','Echo','Alexa','Amazon Athena','Amazon Connect', 'AWS QuickSight', 'Sagemaker','Echo', 'Athena',\n",
        "                     'Chime', 'Slack', 'Facebook','Workplace','Facebook Messenger','WhatsApp', 'Rooms','Oculus','fb.gg','Facebook Gaming',\n",
        "                     'Google', 'Android','Google Assistant','Google Home','Google Cloud','GSuite','G Suite','Gsuite','G suite',\n",
        "                     'Gmail','Chromebook','Jamboard','Pixel', 'Google Classroom','Hangouts','Google VR','Google Daydream','Google Drive','Google Glass',\n",
        "                     'Stadia','Google BigQuery','Google BigTable', 'Google Cloud Spanner','Google Data Studio','Google Meet','Google Workspace', 'IBM',\n",
        "                     'Watson','Watson for Oncology','IBM Cloud', 'Cognitiva', 'Red Hat', 'BlueMix', 'ZOOM','Zoom','zoom']\n",
        "    return words_to_verify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhtcLjxt0RFs"
      },
      "source": [
        "##Define Words SOV Focused"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wguYo5B50F6P"
      },
      "source": [
        "def define_words_sov_focused():\n",
        "    sov_focused=['Microsoft','Office 365','Cortana', 'Dynamics', 'Azure Cosmos', 'O365',  'Microsoft Cognitive',  'Internet Explorer',  'Minecraft',\n",
        "                     'Windows Server','Github','GitHub','Scarlett','Power BI', 'Edge', 'OneNote',  'PowerPoint',  'Windows',  'Power Point', \n",
        "                     'Office', 'Cortana Intelligence Suite', 'Power Apps', 'LinkedIn', 'Age of Empires','ID@Xbox','SharePoint', 'Xbox Project Scarlett', 'Xbox One','Xbox Scarlett',\n",
        "                     'Surface Pro', 'Surface', 'Azure', 'OneDrive', 'Outlook', 'SQL Server', 'Xbox One','Microsoft Flight Simulator', 'Halo', 'Word',\n",
        "                     'Paint',  'Excel',  'Xbox',  'One Drive',  'Azure Al',  'HoloLens',  'Microsoft Bot Framework',  'Teams', 'TEAMS','Windows Defender',\n",
        "                     'Skype', 'Skype for Business','Apple','Mac','OSX','Siri','iMac','HomePod',\n",
        "                     'ARKit','MacOS','Macbook','FaceTime','tvOS','SwiftUI', 'Amazon','Amazon Web Services',\n",
        "                     'AWS','AWS Educate','Echo','Alexa','Amazon Athena','Amazon Connect', 'AWS QuickSight', 'Sagemaker','Echo', 'Athena',\n",
        "                     'Chime', 'Slack', 'Facebook','Workplace','Facebook Messenger', 'Rooms','Oculus','fb.gg','Facebook Gaming',\n",
        "                     'Google','Google Assistant','Google Home','Google Cloud','GSuite','G Suite','Gsuite','G suite',\n",
        "                     'Gmail','Chromebook','Jamboard','Google Classroom','Hangouts','Google VR','Google Daydream','Google Drive','Google Glass',\n",
        "                     'Stadia','Google BigQuery','Google BigTable', 'Google Cloud Spanner','Google Data Studio','Google Meet','Google Workspace', 'IBM',\n",
        "                     'Watson','Watson for Oncology','IBM Cloud', 'Cognitiva', 'Red Hat', 'BlueMix', 'ZOOM','Zoom','zoom']\n",
        "    return sov_focused"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxAiAEip0Yhi"
      },
      "source": [
        "##Define Customer Name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1lODLI60VnM"
      },
      "source": [
        "def define_customer_name():\n",
        "    customer_name=['Ministério Público do Rio de Janeiro (MPRJ)', 'Best Buy', 'MKDA', 'Flecha roja', 'Deutsche Bank', 'Synnex Westcon-Comstor', 'Banpara', 'TP Digital', 'UNIFIN', 'Fundación Alcaraván',\n",
        "                     'Universidad Externado de Colombia', 'PBSF', 'The Ministry of Education (Minedu)', 'Banco Hipotecario', 'German School', 'Liverpool', 'BUAP', 'EY Chile', 'Arcor', 'Secretaria Estadual de Educação (Seduc)',\n",
        "                     'Brasoftware y Odebrecht', 'Eatfit University','F1', 'Unilever', 'Government of Rio Grande do Sul', 'Itaú', 'Jabra PanaCast', 'Grupo Açotubo', 'Rede D’Or','Secretaria Estadual de Educação de Parana',\n",
        "                     'The System of Emergency Medical Care (SAME)', 'BMW', 'Government of Mexico', 'Coursera', 'Fórmula 1', 'Dell', 'Workday', 'Commvault', 'Colombian Chamber of Electronic Commerce', 'Colegio Jacarandá',\n",
        "                     'Swiss Medical', 'Kindite', 'Claro', 'USA Department of Defense', 'Fiat Chrysler', 'Mi Tierrita', 'Conselho Nacional de Justiça', 'Nutanix', 'Deutsche Bank', 'SOU.cloud',\n",
        "                     'Eco-Kindergarten Mi Tierrita', 'The Latin University', 'Corte Suprema', 'The Tax Administration Service (SAT)', 'Telefonica', 'Banco Sabadell', 'Toyota', 'The Court of Justice of the State of São Paulo (TJSP)',\n",
        "                     'Carlsberg', 'Méderi', 'Business Data Evolution', 'The Autonomous University of Nuevo León (UANL)', 'Government of Guatemala', 'Odecma', 'The Autonomous University of Tamaulipas (UAT)', 'Santo Tomas de Aquino University (USTA)',\n",
        "                     'HSBC', 'Teleperformance', 'Ministry of Industry, Commerce and SMBs (MICM)', 'TCS', 'ADT', 'Department of Education (DE)', 'Fundação Vale', 'Tirando x Colombia', 'OEA', 'Municipality of Providencia',\n",
        "                     'The College of Professionals in Informatics and Computing (CPIC)', 'Roomie', 'Colombian Chamber of Electronic Commerce (CCCE)', 'Baires Rocks', 'Banco Comafi', 'Central American Bottling Company (CBC)',\n",
        "                     'Ray-ban', 'Grupo Bolívar Davivienda','Temasek','Andino School', 'BBVA', 'Grupo Atlas de Seguridad Integral', 'Manzanillo port', 'Critertec', 'Etsy', 'The Chamber of Chinese Companies in Chile (CECC)',\n",
        "                     'American Airlines', 'Red pública estatal de Rio Grande do Sul', 'Nexsys', 'The Ministry of Education of Panama', 'Air Computers', 'Casa Blanca', 'S4Go',\n",
        "                     'The Ministry of Education', 'AT&T','CSJJ', 'Banco BBVA', 'Universidad Anahuac Puebla', 'Atento', 'Sapore', 'Lopez y Asociados', 'Pentagon', 'Foro de Periodismo Argentino',\n",
        "                     'Yapp', 'MELI', 'Panamanian Chamber of Commerce, Industries and Agriculture (CCIAP)', 'SEP', 'Loja Integrada', 'Algramo', 'Coppel', 'Cisco Meraki','Reliance',\n",
        "                     'SEAQ', 'Centro Australiano de Egiptologia da Macquarie University', 'Secretaria de Governo Digital (SGD)', 'WizdomCRM', 'Orange', 'Universidad Continental', 'Fopea', 'ClearSale', 'Certsys',\n",
        "                     'The National Polytechnic Institute (IPN)', 'El Instituto de Radiología (InRad), InovaHC y la Universidad de São Paulo (HCFMUSP)', 'NEO Consulting', 'SAS', 'Arezzo',\n",
        "                     'Canonical', 'The Technological University of El Salvador (UTEC)', 'Lenovo DCG', 'Renault', 'Government of Chile', 'Samsung', 'São Luiz Hospital', 'Crea', 'Citrix', 'Medicos sin Fronteras',\n",
        "                     'Hospital Albert Einstein', 'National Learning Service (SENA)', 'CNJ', 'NFL', 'Benemérita Universidad Autónoma de Puebla', 'Petrobras', 'Porto Seguro', 'Corte Superior de Justicia de Junín',\n",
        "                     'Universidad Insurgentes (UIN)','GBM','Kaizen', 'Konecta', 'Government', 'Starbucks', 'Aleph CRM', 'Central Board of Secondary Education (CBSE)', 'Preparatoria Venustiano Carranza', \n",
        "                     'Central American Bottling Company', 'Hospital network Einstein','Digital Innovation One', 'KIA', 'Mercedes-Benz', 'MSCI Inc.', 'Universidad Federal de Paraíba (UFPB)', 'Control de la Magistratura',\n",
        "                     'UISA', 'Instituto Nacional de México (ITA)', 'University of Puerto Rico (UPR)', 'Movistar', 'Department of Agriculture', 'Dedalus', 'Lenovo', 'Ministry of information and communication technologies (MinTIC)',\n",
        "                     'Plan Ceibal', 'Ministry of Education', 'University Action Pro Education and Culture', 'Cmind', 'Webee', 'Moderna', 'Adobe', 'Mastercard', 'Government of Colombia', 'Zenvia', 'NBA']\n",
        "    return customer_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYefL1UX0h4S"
      },
      "source": [
        "##Define Cities & People"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAF3gsKk0cWa"
      },
      "source": [
        "def define_cities_people():\n",
        "    cities_people=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaXf0na40tqL"
      },
      "source": [
        "##Identify Presence Of (Brand Or/And Products) In Title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPqTeJx70lWL"
      },
      "source": [
        "def wtv_in_title(row, wtv):\n",
        "    encontradas=[]\n",
        "    for item in wtv:\n",
        "        if item in row:\n",
        "            encontradas.append(item)\n",
        "        else:\n",
        "            continue\n",
        "    if len(encontradas)>0:\n",
        "        #print('Nota relacionada')\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        #print('Nota no relacionada')\n",
        "        return 'No'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP3ZHyEb1Hsa"
      },
      "source": [
        "##Identify Presence Of (Brand Or/And Products) In Content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alaoR7OA1H13"
      },
      "source": [
        "def wtv_in_content(row,wtv):\n",
        "    content_a_revisar=['Amazon','Facebook', 'WhatsApp']\n",
        "    encontradas=[]\n",
        "    revisar=[]\n",
        "    for item in wtv:\n",
        "        if item in row:\n",
        "            encontradas.append(item)\n",
        "            if item in content_a_revisar:\n",
        "                revisar.append(item)\n",
        "        else:\n",
        "            continue\n",
        "    if len(encontradas)>0 and len(revisar)>0:\n",
        "        #print('Nota con wtv y content a revisar ')\n",
        "        return 'Revisar'\n",
        "    elif len(encontradas)>0 and len(revisar)==0:\n",
        "        #print('Nota con wtv dentro de content')\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        #print('Wtv no encontrada dentro del contenido')\n",
        "        return 'No'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewkZNEj81ipz"
      },
      "source": [
        "##Function To Verify Related Notes: Based On Mentions In Title & Contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e-aXomC1i6w"
      },
      "source": [
        "def verify_related_overall(row1,row2):\n",
        "    if row1=='Yes' and row2=='Yes':\n",
        "        return 'Yes ->Title & Contents'\n",
        "    elif row1=='Yes' and row2=='Revisar':\n",
        "        return 'Yes ->Revisar Contents'\n",
        "    elif row1=='Yes' and row2=='No':\n",
        "        return 'Yes -> Title pero no en Contents'\n",
        "    elif row1=='No' and row2=='Yes':\n",
        "        return 'Yes -> solo Contents'\n",
        "    elif row1=='No' and row2=='Revisar':\n",
        "        return 'No -> revisar Contents'\n",
        "    elif row1=='No' and row2=='No':\n",
        "        return 'No relacionada'\n",
        "    else:\n",
        "        return 'Combinación inválida'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJN9YLzs17uM"
      },
      "source": [
        "def verify_is_related(row1):\n",
        "    if row1=='Yes ->Title & Contents':\n",
        "        return 'Yes'\n",
        "    elif row1=='Yes ->Revisar Contents':\n",
        "        return 'Review'\n",
        "    elif row1=='Yes -> Title pero no en Contents':\n",
        "        return 'Review'\n",
        "    elif row1=='Yes -> solo Contents':\n",
        "        return 'Yes'\n",
        "    elif row1=='No -> revisar Contents':\n",
        "        return 'Review'\n",
        "    elif row1=='No relacionada':\n",
        "        return 'No'\n",
        "    else:\n",
        "        return 'Combinación inválida'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NPN0kG12RaC"
      },
      "source": [
        "##Function To Verify Company Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPDjM1FR2Qf-"
      },
      "source": [
        "#array(['Non_related', 'Passive', 'Relevant', 'Prominent'], dtype=object)\n",
        "def company_sentiment(row1):\n",
        "    if row1=='Non_related':\n",
        "        return 'NA'\n",
        "    elif row1=='Passive':\n",
        "        return 'Neutral'\n",
        "    elif row1=='Relevant':\n",
        "        return 'Positive'\n",
        "    elif row1=='Prominent':\n",
        "        return 'Positive'\n",
        "    else:\n",
        "        return 'Combinación inválida'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHNoL9aT2qIS"
      },
      "source": [
        "##Function To Verify LOCAL/GLOBAL Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGwh7Mfh2f8U"
      },
      "source": [
        "#if len(row)>0 then 'Local' else 'Global'\n",
        "def verify_local_global(row1):\n",
        "    if len(row1)>0:\n",
        "        return 'Local'\n",
        "    elif len(row1)==0:\n",
        "        return 'Global'\n",
        "    else:\n",
        "        return 'Combinación inválida'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB73WRt73JO9"
      },
      "source": [
        "##Function To Verify DB (Direct Business) AND NDB (Non Direct Business) Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrL2P15r3JvM"
      },
      "source": [
        "#### BUSINESS IN CONTENT\n",
        "def verify_business_in_content(row1,row2):\n",
        "    business_a_revisar=['Android', 'Pixel', 'Prime Video', 'iPhone', 'iOS', 'Airpods','Bing']\n",
        "    encontradas=[]\n",
        "    revisar=[]\n",
        "    for business in business_a_revisar:\n",
        "        if business in row1 or business in row2:\n",
        "            encontradas.append(business)\n",
        "        else:\n",
        "            continue\n",
        "    if len(encontradas)>0 :\n",
        "        #print('Nota con wtv y content a revisar ')\n",
        "        return 'NDB'\n",
        "    elif len(encontradas)==0:\n",
        "        #print('Nota con wtv dentro de content')\n",
        "        return 'DB'\n",
        "    else:\n",
        "        #print('Wtv no encontrada dentro del contenido')\n",
        "        return 'Combinación inválida'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOPglDzp3fT8"
      },
      "source": [
        "##Function To Verify SOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLi7SfBQ3brn"
      },
      "source": [
        "def verify_sov(row1,row2,wtv):\n",
        "    encontradas=[]\n",
        "    for item in wtv:\n",
        "        if item in row1 or item in row2:\n",
        "            encontradas.append(item)\n",
        "        else:\n",
        "            continue\n",
        "    if len(encontradas)>0 :\n",
        "        #print('Nota con wtv y content a revisar ')\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        #print('Wtv no encontrada dentro del contenido')\n",
        "        return 'No'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn-LRk2b38dT"
      },
      "source": [
        "##Function To Verify Customer Name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G29xm5038sG"
      },
      "source": [
        "def verify_customer_name(row1,row2,customer_name):\n",
        "    encontradas=[]\n",
        "    for item in customer_name:\n",
        "        if item in row1 or item in row2:\n",
        "            encontradas.append(item)\n",
        "        else:\n",
        "            continue\n",
        "    if len(encontradas)>0 :\n",
        "        #print('Nota con wtv y content a revisar ')\n",
        "        return 'Yes'\n",
        "    else:\n",
        "        #print('Wtv no encontrada dentro del contenido')\n",
        "        return 'No'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}