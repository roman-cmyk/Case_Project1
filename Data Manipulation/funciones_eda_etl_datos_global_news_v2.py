# -*- coding: utf-8 -*-
"""Funciones_EDA_ETL_Datos_Global_News_V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12q90xIFurmMCKRMrbHAolTtUpjeyK-eK

##Time Stamp Funcion
"""

def get_timestampYMD():
    import datetime
    return datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')

"""##Load Stopwords"""

def define_stopwords():
    sw_es=stopwords.words('spanish')
    sw_pt=stopwords.words('portuguese')
    sw_en=stopwords.words('english')
    sw_es_title=[word.title() for word in sw_es]
    sw_pt_title=[word.title() for word in sw_pt]
    sw_en_title=[word.title() for word in sw_en]
    return sw_es, sw_pt, sw_en, sw_es_title, sw_pt_title, sw_en_title

"""##Extract Uppercase Words"""

def get_mayusculas(texto):
    texto=str(texto)
    palabras_importantes=[]
    mayusculas=(r"([A-Z][a-zÁ-ÿ0-9]{1,20}\s?\,?\.?\s?)")
    texto=re.sub('[^\w\s]',' ',texto)
    texto=re.sub('[0-9]+', '', texto)  
    tokenizer=RegexpTokenizer(r'\w+')
    texto=tokenizer.tokenize(texto)
    texto=[word for word in texto if word not in sw_pt]
    texto=[word for word in texto if word not in sw_es]
    texto=[word for word in texto if word not in sw_en]
    texto=[word for word in texto if word not in sw_pt_title]
    texto=[word for word in texto if word not in sw_es_title]
    texto=[word for word in texto if word not in sw_en_title]
    tokens=[word.strip() for word in texto if word is not None]
    tokens=[word.strip() for word in tokens if len(word)>1]
    palabras_importantes=[word for word in tokens if word.istitle()]
    return palabras_importantes

"""##Adjust Country & Subsidiary"""

def country_sub(country):
    if country =='Panama' or country =='PA' or country=='panama' or country=='Panamá':
        return ['Panama','Central']
    elif country =='Costa Rica' or country =='CR' or country=='costa rica':
        return ['Costa Rica','Central']
    elif country =='Honduras' or country =='HN' or country=='honduras':
        return ['Honduras','Central']
    elif country =='Nicaragua' or country =='NI' or country=='nicaragua':
        return ['Nicaragua','Central']
    elif country =='Guatemala' or country =='GT' or country=='guatemala':
        return ['Guatemala','Central']
    elif country =='El Salvador' or country =='SV' or country=='el salvador':
        return ['El Salvador','Central']
    elif country =='Venezuela' or country =='VE' or country=='venezuela':
        return ['Venezuela','Central']
    elif country =='Puerto Rico' or country =='PR' or country=='puerto rico':
        return ['Puerto Rico','Caribbean']
    elif country =='Dominican Republic' or country =='DO' or country=='dominican republic' or country=='republica dominicana' or country=='Rep. Dominicana':
        return ['Dominican Republic','Caribbean']
    elif country =='Trinidad & Tobago' or country =='TT' or country=='trinidad & tobago' or country=='Trinidad y Tobago' or country=='T&T':
        return ['Trinidad & Tobago','Caribbean']
    elif country =='Jamaica' or country =='JM' or country=='jamaica':
        return ['Jamaica','Caribbean']
    elif country =='Peru' or country =='PE' or country=='peru' or country=='Perú':
        return ['Peru','South']
    elif country =='Ecuador' or country =='EC' or country=='ecuador' or country=='equador' or country=='EQUADOR':
        return ['Ecuador','South']
    elif country =='Bolivia' or country =='BO' or country=='bolivia':
        return ['Bolivia','South']
    elif country =='Paraguay' or country =='PY' or country=='paraguay' or country=='paraguai' or country=='Paraguai':
        return ['Paraguay','South']
    elif country =='Uruguay' or country =='UY' or country=='uruguay'or country=='uruguai' or country=='Uruguai':
        return ['Uruguay','South']
    elif country =='Mexico' or country =='MX' or country=='mexico' or country=='México' or country=='méxico':
        return ['Mexico','Mexico']
    elif country =='PanLatam' or country =='panlatam' or country =='Latinoamérica':
        return ['PanLatam','Panlatam']
    elif country =='PanCentral' or country =='pancentral':
        return ['PanCentral','Panlatam']
    elif country =='Argentina' or country =='AR' or country=='argentina':
        return ['Argentina','Argentina']
    elif country =='Brazil' or country =='BR' or country=='brazil' or country=='brasil' or country=='Brasil':
        return ['Brazil','Brazil']
    elif country =='Chile' or country =='CL' or country=='chile':
        return ['Chile','Chile']
    elif country =='Colombia' or country =='CL' or country=='colombia':
        return ['Colombia','Colombia']
    else:
        return ['Revisar','Revisar']

"""##Adjust Month & Quarters"""

def find_quarters(month):
    if month =='1': 
        return ['January','07','Q3','H2']
    elif month =='2': 
        return ['February','08','Q3','H2']
    elif month =='3': 
        return ['March','09','Q3','H2']
    elif month =='4': 
        return ['April','10','Q4','H2']
    elif month =='5': 
        return ['May','11','Q4','H2']
    elif month =='6': 
        return ['June','12','Q4','H2']
    elif month =='7': 
        return ['July','01','Q1','H2']
    elif month =='8': 
        return ['August','02','Q1','H2']
    elif month =='9': 
        return ['September','03','Q1','H2']
    elif month =='10': 
        return ['October','04','Q2','H2']
    elif month =='11': 
        return ['November','05','Q2','H2']
    elif month =='12': 
        return ['December','06','Q2','H2']

"""##Remove Punctuation Marks"""

def remove_punct(texto):
    try:
        texto=texto.replace(".",' ').replace(";",' ').replace(":",' ').replace(",",' ')
        texto=texto.replace("(",' ').replace(")",' ').replace("|",' ').replace('"',' ')
        texto=texto.replace("%",' ').replace("$",' ').replace("/",' ').replace('\'',' ')
        texto=texto.replace("-",' ').replace("_",' ').replace("*",' ').replace('+',' ')
        texto=texto.replace("#",' ').replace("@",' ').replace("!",' ').replace('?',' ')
    except:
        pass
    return texto

"""##Convert Text To Lowercase"""

def clean_text(texto):
    texto=texto.lower()
    texto=remove_punct(texto)
    return texto

"""##Text Tokenization"""

def tokenizar(texto):
    tokens = [t for t in texto.split()]
    return tokens

"""##Build Keywords Dictionary"""

def crear_dicc_keywords(df_keywords):
    df_keywords=df_keywords.fillna('exxxtract')
    area_dict = df_keywords.to_dict('list')
    for k,v in area_dict.items():
        nv=list(set(v))
        nv=[x for x in v if x != 'exxxtract']
        nv=list(set(nv))
        area_dict[k]=nv
    return area_dict

"""##Convert Identified Words To a List"""

def convert_to_words(list):
    new_list=[]
    for item,item2 in zip(list,category_list_found):
        if int(item)>0: 
            new_list.append(item2.replace('_FOUND',''))
    return new_list

"""##Read .JSON Files With Encoding 'UTF-8'"""

def read_json(file_path):
    data={}
    with open(file_path,encoding="utf8") as json_file:
        arq = json.load(json_file)
    data['Notas']=arq
    return data

"""##Create Empty DataFrame To Save Global News Data"""

def crear_df_json(data):                                                                                  
    df_n=pd.DataFrame(columns=['IdNoticia', 'Fecha', 'Hora', 'TipoDeMedio', 'Medio', 'País', 'Sección','Título', 'Cuerpo', 'Tier',
                               'NroCaracteres', 'Tono', 'LinkImagen','CPE', 'Moneda', 'Audiencia', 'Tema', 'Empresa', 'NroPagina',
                               'Dimensión', 'CirculacionMedio', 'AutorConductor', 'ResumenAclaracion','LinkNota'])
    for item in data['Notas']:                                                                            
        dftemp=pd.DataFrame.from_dict(item,orient='index')                                                
        dftemp=dftemp.T                                                                                   
        df_n=df_n.append(dftemp)                                                                          
    df_n=df_n.reset_index()                                                                               
    return df_n

"""##Find Out a Single Word In Tokenized Text"""

def find_single_word_in_tokenized_text(row,keyword):
    if len([x for x in row if x ==keyword])>=1: 
        return [x for x in row if x ==keyword] 
    else:
        return ' '

"""##Function To Categorize Notes"""

def categoriza_nota(Brands,Priority_products,text):
    text=str(text)
    text=text.replace('[','').replace(']','')
    menciones=re.findall(Brands,text)
    if menciones:
        dicc_menciones={}
        dicc_menciones[list(set(menciones))[0]]=len(menciones)
    else:
        dicc_menciones={}
    productos_enc=[]
    for item in Priority_products:
        if item:
            p_enc=re.findall(item,text)
            productos_enc.append(p_enc)
        else:
            None
    dicc={}
    for item2 in productos_enc:
        if item2:
            dicc[list(set(item2))[0]]=len(item2)
        else:
            None
    flat_products = [x for sublist in productos_enc for x in sublist]
    cuenta=len(menciones)+len(flat_products)
    if cuenta>=3:
        cat_nota='Prominent'
    elif cuenta>1 and cuenta<3:
        cat_nota='Relevant'
    elif cuenta==1:
        cat_nota='Passive'
    else:
        cat_nota='Non_related'
    return dicc_menciones,dicc,cat_nota

# Mas de 3 menciones de producto+marca = Prominent
# 1<mención de Marca + Producto<3 = Relevant
# 1=Pasive
# 0=Null

"""##Define User Agent List"""

import urllib.request
import random

def define_user_agent_list():
    user_agent_list = [
       #Chrome
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
        'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
        #Firefox
        'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',
        'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',
        'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',
        'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',
        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',
        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'
    ]
    return user_agent_list

"""##Get Full Content Using Web Scraping With BeautifulSoup & Urllib"""

def get_full_content(url):
    for i in range(1,6):
    #Pick a random user agent
        user_agent = random.choice(user_agent_list)
        #Set the headers 
        headers = {'User-Agent': user_agent}

        if url:
            try: 
                req = urllib.request.Request(url,headers={'User-Agent': user_agent})
                response = urllib.request.urlopen(req)
                html = response.read()
                 
                soup = BeautifulSoup(html)
                paragraphs=re.findall(r'<p>(.*?)</p>',str(soup))
                paragraphs=re.sub(r'<a href=.+?(?=)>|</a>|\xa0|<strong>|</strong>|<i(.*?)</i>|<img(.*?)>','',str(paragraphs))
                paragraphs2=''.join(paragraphs)
                paragraphs2=paragraphs2.replace('\n','').replace('\t','').replace('\r','')
            except:
                paragraphs2='Sin Informacion'
                print('ups')

    return(paragraphs2)

"""##Clean & Tokenize Text"""

def clean_and_tokenize(texto):
    texto=str(texto).lower()
    texto=re.sub('[^\w\s]',' ',texto)
    tokenizer=RegexpTokenizer(r'\w+')
    texto=tokenizer.tokenize(texto)
    texto=[word for word in texto if word not in sw_pt]
    texto=[word for word in texto if word not in sw_es] #Limpiar stopwords en español
    texto=[word for word in texto if word not in swsp] #Limpiar stopwords en español
    tokens=[word.strip() for word in texto if word is not None]
    tokens=[word.strip() for word in tokens if len(word)>1]
    return tokens

"""##Find Most Common Words"""

def find_most_common(texto,filtradas,qty):
    tokens=clean_and_tokenize(texto)
    tokens2=[word for word in tokens if word not in filtradas]
    bigrm = list(nltk.bigrams(tokens2))
    found=collections.Counter(bigrm)
    tops=found.most_common(qty)
    mf=[]
    for i in range(0,len(tops)):
        terms=tops[i][0]
        freq=tops[i][1]
        most_frequent=(terms,freq)
        mf.append(most_frequent)
    return mf

"""##Find Microsoft Most Common Words"""

def MC_WORDS(df,filtro_company,qty):
    filtradas=[]
    df['Contents_MC']=df['Contents'].apply(lambda row : str(row).replace('http://','').replace('https://','').replace('www',''))
    for filtro in filtro_company:
        df['Contents_MC']=df['Contents'].apply(lambda row : str(row).replace(filtro,''))
    lista02=df['Contents'].tolist()
    mc03=find_most_common(lista02,filtradas,qty)
    return mc03

def MC_WORDS2(df,filtro_company,qty):
    filtradas=[]
    df['Full Text_MC']=df['Full Text'].apply(lambda row : str(row).replace('http://','').replace('https://','').replace('www',''))
    for filtro in swsp:
        df['Full Text_MC']=df['Full Text'].apply(lambda row : str(row).replace(filtro,''))
    lista02=df['Full Text_MC'].tolist()
    mc03=find_most_common(lista02,filtradas,qty)
    return mc03

"""##Define Spanish Stopwords"""

def define_swsp():

    swsp=['de','la','que','el','en','y','a','los','del','se','las','por','un','para','con','no',
          'una','su','al','lo','como','más','pero','sus','le','ya','o','este','sí','porque','esta',
          'entre','cuando','muy','sin','sobre','también','me','hasta','hay','donde','quien','desde',
          'todo','nos','durante','todos','uno','les','ni','contra','otros','ese','eso','ante','ellos',
          'e','esto','mí','antes','algunos','qué','unos','yo','otro','otras','otra','él','tanto','esa',
          'estos','mucho','quienes','nada','muchos','cual','poco','ella','estar','estas','algunas','algo',
          'nosotros','mi','mis','tú','te','ti','tu','tus','ellas','nosotras','vosostros','vosostras','os',
          'mío','mía','míos','mías','tuyo','tuya','tuyos','tuyas','suyo','suya','suyos','suyas','nuestro',
          'nuestra','nuestros','nuestras','vuestro','vuestra','vuestros','vuestras','esos','esas','estoy',
          'estás','está','estamos','estáis','están','esté','estés','estemos','estéis','estén','estaré',
          'estarás','estará','estaremos','estaréis','estarán','estaría','estarías','estaríamos','estaríais',
          'estarían','estaba','estabas','estábamos','estabais','estaban','estuve','estuviste','estuvo',
          'estuvimos','estuvisteis','estuvieron','estuviera','estuvieras','estuviéramos','estuvierais',
          'estuvieran','estuviese','estuvieses','estuviésemos','estuvieseis','estuviesen','estando','estado',
          'estada','estados','estadas','estad','he','has','ha','hemos','habéis','han','haya','hayas',
          'hayamos','hayáis','hayan','habré','habrás','habrá','habremos','habréis','habrán','habría',
          'habrías','habríamos','habríais','habrían','había','habías','habíamos','habíais','habían','hube',
          'hubiste','hubo','hubimos','hubisteis','hubieron','hubiera','hubieras','hubiéramos','hubierais',
          'hubieran','hubiese','hubieses','hubiésemos','hubieseis','hubiesen','habiendo','habido','habida',
          'habidos','habidas','soy','eres','es','somos','sois','son','sea','seas','seamos','seáis','sean',
          'seré','serás','será','seremos','seréis','serán','sería','serías','seríamos','seríais','serían',
          'era','eras','éramos','erais','eran','fui','fuiste','fue','fuimos','fuisteis','fueron','fuera',
          'fueras','fuéramos','fuerais','fueran','fuese','fueses','fuésemos','fueseis','fuesen','sintiendo',
          'sentido','sentida','sentidos','sentidas','siente','sentid','tengo','tienes','tiene','tenemos',
          'tenéis','tienen','tenga','tengas','tengamos','tengáis','tengan','tendré','tendrás','tendrá',
          'tendremos','tendréis','tendrán','tendría','tendrías','tendríamos','tendríais','tendrían','tenía',
          'tenías','teníamos','teníais','tenían','tuve','tuviste','tuvo','tuvimos','tuvisteis','tuvieron',
          'tuviera','tuvieras','tuviéramos','tuvierais','tuvieran','tuviese','tuvieses','tuviésemos',
          'tuvieseis','tuviesen','teniendo','tenido','tenida','tenidos','tenidas','tened','https','co','mucha','rt','poner',
          'interlocutor','interlocutora','presidente','AMLO','Andrés Manuel López Obrador','Andres Manuel Lopez Obrador',
          'Andres Manuel','Lopez Obrador','Andrés Manuel','López Obrador','PRESIDENTE ANDRÉS MANUEL LÓPEZ OBRADOR',
          'INTERLOCUTORA','INTERLOCUTOR','andrés','manuel','lópez','obrador','interlocutor','interlocutora','br']
    return swsp

"""##Remove Spanish Stopwords"""

def remove_swsp(texto):                    
    tokens = [t for t in texto.split()]
    clean_tokens = tokens[:]
    for token in tokens:
        if token in swsp:
            clean_tokens.remove(token)
    return clean_tokens

"""##Split The Text In N-Grams"""

def clean_text_wt(texto):
    clean_tokens=clean_and_tokenize(texto)
    texto = ' '.join(clean_tokens)
    return texto

def divide_en_ngramas(texto,n):
    if len(texto)>0:
        n_grams=[]
        enegramas = ngrams(texto.split(), n)
        for grams in enegramas:
            i=0
            division=''
            while i<n: 
                division=division+' '+grams[i]
                i+=1
            n_grams.append(division.lstrip())
    else:
        return ''
    return n_grams

"""##Detect N-Grams In Text"""

def detect_ngram_in_text(texto,keyword):
    tam_keyword=len(keyword.split(' '))
    if tam_keyword==1:

        texto_fuente=clean_and_tokenize(texto)
        if keyword in texto_fuente:
            return 1
        else: 
            return 0
    else:
        texto_fuente=divide_en_ngramas(texto,tam_keyword)
        if keyword in texto_fuente:
            return 1
        else: 
            return 0

def detect_ngram_in_text2(texto,keyword):
    tam_keyword=len(keyword.split(' '))
    if tam_keyword==1:

        texto_fuente=clean_and_tokenize(texto)
        if keyword in texto_fuente:
            return 1
        else: 
            return 0
    else:
        texto_fuente=divide_en_ngramas(texto,tam_keyword)
        if keyword in texto_fuente:
            return 1
        else: 
            return 0

"""##Find Out Best Matches Between Keywords & Words In Content"""

def encuentra_matches(row,keyword):
    row=str(row)
    matched=process.extract(keyword,row.split(' '))
    best_options=[]
    for item in matched:
        if item[1]>=91:
            best_options.append(item[0])
    return best_options

def find_keyword_in_clean_text(row,keyword):
    patterns= [keyword]  
    for p in patterns:
        match= re.findall(p, row)
    return match

"""##Define Words To Verify"""

def define_words_to_verify():
    words_to_verify=['Microsoft','Office 365', 'Bing', 'Cortana', 'Dynamics', 'Azure Cosmos', 'O365',  'Microsoft Cognitive',  'Internet Explorer',  'Minecraft',
                     'Windows Server','Github','GitHub','Scarlett','Power BI', 'Edge', 'OneNote',  'PowerPoint',  'Windows',  'Power Point', 
                     'Office', 'Cortana Intelligence Suite', 'Power Apps', 'LinkedIn', 'Age of Empires','ID@Xbox','SharePoint', 'Xbox Project Scarlett', 'Xbox One','Xbox Scarlett',
                     'Surface Pro', 'Surface', 'Azure', 'OneDrive', 'Outlook', 'SQL Server', 'Xbox One','Microsoft Flight Simulator', 'Halo', 'Word',
                     'Paint',  'Excel',  'Xbox',  'One Drive',  'Azure Al',  'HoloLens',  'Microsoft Bot Framework',  'Teams', 'TEAMS','Windows Defender',
                     'Skype', 'Skype for Business','Apple','iPad','Mac','OSX','Siri','iPhone','iMac','HomePod',
                     'ARKit','MacOS','Macbook','watchOS','Airpods','FaceTime','tvOS','SwiftUI', 'Amazon','Amazon Web Services',
                     'AWS','AWS Educate','Echo','Alexa','Amazon Athena','Amazon Connect', 'AWS QuickSight', 'Sagemaker','Echo', 'Athena',
                     'Chime', 'Slack', 'Facebook','Workplace','Facebook Messenger','WhatsApp', 'Rooms','Oculus','fb.gg','Facebook Gaming',
                     'Google', 'Android','Google Assistant','Google Home','Google Cloud','GSuite','G Suite','Gsuite','G suite',
                     'Gmail','Chromebook','Jamboard','Pixel', 'Google Classroom','Hangouts','Google VR','Google Daydream','Google Drive','Google Glass',
                     'Stadia','Google BigQuery','Google BigTable', 'Google Cloud Spanner','Google Data Studio','Google Meet','Google Workspace', 'IBM',
                     'Watson','Watson for Oncology','IBM Cloud', 'Cognitiva', 'Red Hat', 'BlueMix', 'ZOOM','Zoom','zoom']
    return words_to_verify

"""##Define Words SOV Focused"""

def define_words_sov_focused():
    sov_focused=['Microsoft','Office 365','Cortana', 'Dynamics', 'Azure Cosmos', 'O365',  'Microsoft Cognitive',  'Internet Explorer',  'Minecraft',
                     'Windows Server','Github','GitHub','Scarlett','Power BI', 'Edge', 'OneNote',  'PowerPoint',  'Windows',  'Power Point', 
                     'Office', 'Cortana Intelligence Suite', 'Power Apps', 'LinkedIn', 'Age of Empires','ID@Xbox','SharePoint', 'Xbox Project Scarlett', 'Xbox One','Xbox Scarlett',
                     'Surface Pro', 'Surface', 'Azure', 'OneDrive', 'Outlook', 'SQL Server', 'Xbox One','Microsoft Flight Simulator', 'Halo', 'Word',
                     'Paint',  'Excel',  'Xbox',  'One Drive',  'Azure Al',  'HoloLens',  'Microsoft Bot Framework',  'Teams', 'TEAMS','Windows Defender',
                     'Skype', 'Skype for Business','Apple','Mac','OSX','Siri','iMac','HomePod',
                     'ARKit','MacOS','Macbook','FaceTime','tvOS','SwiftUI', 'Amazon','Amazon Web Services',
                     'AWS','AWS Educate','Echo','Alexa','Amazon Athena','Amazon Connect', 'AWS QuickSight', 'Sagemaker','Echo', 'Athena',
                     'Chime', 'Slack', 'Facebook','Workplace','Facebook Messenger', 'Rooms','Oculus','fb.gg','Facebook Gaming',
                     'Google','Google Assistant','Google Home','Google Cloud','GSuite','G Suite','Gsuite','G suite',
                     'Gmail','Chromebook','Jamboard','Google Classroom','Hangouts','Google VR','Google Daydream','Google Drive','Google Glass',
                     'Stadia','Google BigQuery','Google BigTable', 'Google Cloud Spanner','Google Data Studio','Google Meet','Google Workspace', 'IBM',
                     'Watson','Watson for Oncology','IBM Cloud', 'Cognitiva', 'Red Hat', 'BlueMix', 'ZOOM','Zoom','zoom']
    return sov_focused

"""##Define Customer Name"""

def define_customer_name():
    customer_name=['Ministério Público do Rio de Janeiro (MPRJ)', 'Best Buy', 'MKDA', 'Flecha roja', 'Deutsche Bank', 'Synnex Westcon-Comstor', 'Banpara', 'TP Digital', 'UNIFIN', 'Fundación Alcaraván',
                     'Universidad Externado de Colombia', 'PBSF', 'The Ministry of Education (Minedu)', 'Banco Hipotecario', 'German School', 'Liverpool', 'BUAP', 'EY Chile', 'Arcor', 'Secretaria Estadual de Educação (Seduc)',
                     'Brasoftware y Odebrecht', 'Eatfit University','F1', 'Unilever', 'Government of Rio Grande do Sul', 'Itaú', 'Jabra PanaCast', 'Grupo Açotubo', 'Rede D’Or','Secretaria Estadual de Educação de Parana',
                     'The System of Emergency Medical Care (SAME)', 'BMW', 'Government of Mexico', 'Coursera', 'Fórmula 1', 'Dell', 'Workday', 'Commvault', 'Colombian Chamber of Electronic Commerce', 'Colegio Jacarandá',
                     'Swiss Medical', 'Kindite', 'Claro', 'USA Department of Defense', 'Fiat Chrysler', 'Mi Tierrita', 'Conselho Nacional de Justiça', 'Nutanix', 'Deutsche Bank', 'SOU.cloud',
                     'Eco-Kindergarten Mi Tierrita', 'The Latin University', 'Corte Suprema', 'The Tax Administration Service (SAT)', 'Telefonica', 'Banco Sabadell', 'Toyota', 'The Court of Justice of the State of São Paulo (TJSP)',
                     'Carlsberg', 'Méderi', 'Business Data Evolution', 'The Autonomous University of Nuevo León (UANL)', 'Government of Guatemala', 'Odecma', 'The Autonomous University of Tamaulipas (UAT)', 'Santo Tomas de Aquino University (USTA)',
                     'HSBC', 'Teleperformance', 'Ministry of Industry, Commerce and SMBs (MICM)', 'TCS', 'ADT', 'Department of Education (DE)', 'Fundação Vale', 'Tirando x Colombia', 'OEA', 'Municipality of Providencia',
                     'The College of Professionals in Informatics and Computing (CPIC)', 'Roomie', 'Colombian Chamber of Electronic Commerce (CCCE)', 'Baires Rocks', 'Banco Comafi', 'Central American Bottling Company (CBC)',
                     'Ray-ban', 'Grupo Bolívar Davivienda','Temasek','Andino School', 'BBVA', 'Grupo Atlas de Seguridad Integral', 'Manzanillo port', 'Critertec', 'Etsy', 'The Chamber of Chinese Companies in Chile (CECC)',
                     'American Airlines', 'Red pública estatal de Rio Grande do Sul', 'Nexsys', 'The Ministry of Education of Panama', 'Air Computers', 'Casa Blanca', 'S4Go',
                     'The Ministry of Education', 'AT&T','CSJJ', 'Banco BBVA', 'Universidad Anahuac Puebla', 'Atento', 'Sapore', 'Lopez y Asociados', 'Pentagon', 'Foro de Periodismo Argentino',
                     'Yapp', 'MELI', 'Panamanian Chamber of Commerce, Industries and Agriculture (CCIAP)', 'SEP', 'Loja Integrada', 'Algramo', 'Coppel', 'Cisco Meraki','Reliance',
                     'SEAQ', 'Centro Australiano de Egiptologia da Macquarie University', 'Secretaria de Governo Digital (SGD)', 'WizdomCRM', 'Orange', 'Universidad Continental', 'Fopea', 'ClearSale', 'Certsys',
                     'The National Polytechnic Institute (IPN)', 'El Instituto de Radiología (InRad), InovaHC y la Universidad de São Paulo (HCFMUSP)', 'NEO Consulting', 'SAS', 'Arezzo',
                     'Canonical', 'The Technological University of El Salvador (UTEC)', 'Lenovo DCG', 'Renault', 'Government of Chile', 'Samsung', 'São Luiz Hospital', 'Crea', 'Citrix', 'Medicos sin Fronteras',
                     'Hospital Albert Einstein', 'National Learning Service (SENA)', 'CNJ', 'NFL', 'Benemérita Universidad Autónoma de Puebla', 'Petrobras', 'Porto Seguro', 'Corte Superior de Justicia de Junín',
                     'Universidad Insurgentes (UIN)','GBM','Kaizen', 'Konecta', 'Government', 'Starbucks', 'Aleph CRM', 'Central Board of Secondary Education (CBSE)', 'Preparatoria Venustiano Carranza', 
                     'Central American Bottling Company', 'Hospital network Einstein','Digital Innovation One', 'KIA', 'Mercedes-Benz', 'MSCI Inc.', 'Universidad Federal de Paraíba (UFPB)', 'Control de la Magistratura',
                     'UISA', 'Instituto Nacional de México (ITA)', 'University of Puerto Rico (UPR)', 'Movistar', 'Department of Agriculture', 'Dedalus', 'Lenovo', 'Ministry of information and communication technologies (MinTIC)',
                     'Plan Ceibal', 'Ministry of Education', 'University Action Pro Education and Culture', 'Cmind', 'Webee', 'Moderna', 'Adobe', 'Mastercard', 'Government of Colombia', 'Zenvia', 'NBA']
    return customer_name

"""##Define Cities & People"""

def define_cities_people():
    cities_people=[]

"""##Identify Presence Of (Brand Or/And Products) In Title"""

def wtv_in_title(row, wtv):
    encontradas=[]
    for item in wtv:
        if item in row:
            encontradas.append(item)
        else:
            continue
    if len(encontradas)>0:
        #print('Nota relacionada')
        return 'Yes'
    else:
        #print('Nota no relacionada')
        return 'No'

"""##Identify Presence Of (Brand Or/And Products) In Content"""

def wtv_in_content(row,wtv):
    content_a_revisar=['Amazon','Facebook', 'WhatsApp']
    encontradas=[]
    revisar=[]
    for item in wtv:
        if item in row:
            encontradas.append(item)
            if item in content_a_revisar:
                revisar.append(item)
        else:
            continue
    if len(encontradas)>0 and len(revisar)>0:
        #print('Nota con wtv y content a revisar ')
        return 'Revisar'
    elif len(encontradas)>0 and len(revisar)==0:
        #print('Nota con wtv dentro de content')
        return 'Yes'
    else:
        #print('Wtv no encontrada dentro del contenido')
        return 'No'

"""##Function To Verify Related Notes: Based On Mentions In Title & Contents"""

def verify_related_overall(row1,row2):
    if row1=='Yes' and row2=='Yes':
        return 'Yes ->Title & Contents'
    elif row1=='Yes' and row2=='Revisar':
        return 'Yes ->Revisar Contents'
    elif row1=='Yes' and row2=='No':
        return 'Yes -> Title pero no en Contents'
    elif row1=='No' and row2=='Yes':
        return 'Yes -> solo Contents'
    elif row1=='No' and row2=='Revisar':
        return 'No -> revisar Contents'
    elif row1=='No' and row2=='No':
        return 'No relacionada'
    else:
        return 'Combinación inválida'

def verify_is_related(row1):
    if row1=='Yes ->Title & Contents':
        return 'Yes'
    elif row1=='Yes ->Revisar Contents':
        return 'Review'
    elif row1=='Yes -> Title pero no en Contents':
        return 'Review'
    elif row1=='Yes -> solo Contents':
        return 'Yes'
    elif row1=='No -> revisar Contents':
        return 'Review'
    elif row1=='No relacionada':
        return 'No'
    else:
        return 'Combinación inválida'

"""##Function To Verify Company Sentiment"""

#array(['Non_related', 'Passive', 'Relevant', 'Prominent'], dtype=object)
def company_sentiment(row1):
    if row1=='Non_related':
        return 'NA'
    elif row1=='Passive':
        return 'Neutral'
    elif row1=='Relevant':
        return 'Positive'
    elif row1=='Prominent':
        return 'Positive'
    else:
        return 'Combinación inválida'

"""##Function To Verify LOCAL/GLOBAL Variable"""

#if len(row)>0 then 'Local' else 'Global'
def verify_local_global(row1):
    if len(row1)>0:
        return 'Local'
    elif len(row1)==0:
        return 'Global'
    else:
        return 'Combinación inválida'

"""##Function To Verify DB (Direct Business) AND NDB (Non Direct Business) Variable"""

#### BUSINESS IN CONTENT
def verify_business_in_content(row1,row2):
    business_a_revisar=['Android', 'Pixel', 'Prime Video', 'iPhone', 'iOS', 'Airpods','Bing']
    encontradas=[]
    revisar=[]
    for business in business_a_revisar:
        if business in row1 or business in row2:
            encontradas.append(business)
        else:
            continue
    if len(encontradas)>0 :
        #print('Nota con wtv y content a revisar ')
        return 'NDB'
    elif len(encontradas)==0:
        #print('Nota con wtv dentro de content')
        return 'DB'
    else:
        #print('Wtv no encontrada dentro del contenido')
        return 'Combinación inválida'

"""##Function To Verify SOV"""

def verify_sov(row1,row2,wtv):
    encontradas=[]
    for item in wtv:
        if item in row1 or item in row2:
            encontradas.append(item)
        else:
            continue
    if len(encontradas)>0 :
        #print('Nota con wtv y content a revisar ')
        return 'Yes'
    else:
        #print('Wtv no encontrada dentro del contenido')
        return 'No'

"""##Function To Verify Customer Name"""

def verify_customer_name(row1,row2,customer_name):
    encontradas=[]
    for item in customer_name:
        if item in row1 or item in row2:
            encontradas.append(item)
        else:
            continue
    if len(encontradas)>0 :
        #print('Nota con wtv y content a revisar ')
        return 'Yes'
    else:
        #print('Wtv no encontrada dentro del contenido')
        return 'No'